{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Soheil Esmaeilzadeh <br /> \n",
    "__Date:__ November 6th, 2020 <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Code Exercise\n",
    "\n",
    "#### Write a program that gets a wikipedia article as an input, and recommend a sequence of 10 articles to read.\n",
    "\n",
    "__Input:__ Weblink to a Wikipedia article. <br /> \n",
    "__Output:__ List of 10 weblinks to 10 wikipedia articles.\n",
    "\n",
    "* Notes:  \n",
    " * You can download all the wikipedia articles as a file on web\n",
    " * The problem is intentionally vague, please use your judgment \n",
    " * You can use any programming language, or library that you want \n",
    " * You don’t need to design a GUI, an application in terminal is enough.\n",
    " * You have 3 days to work on but we expect you spend less than 4 hours on it\n",
    " \n",
    "__Example:__ <br />\n",
    "Input: https://en.wikipedia.org/wiki/Snake <br />\n",
    "Output: 10 wikipedia links e.g. https://en.wikipedia.org/wiki/Green_anaconda and 9 more links <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Recommendation Framework\n",
    "\n",
    "### Recommendation Approach\n",
    "Here I build a recommender system based on the very basic principles of Content-Based Recommendation apporach [[1](https://link.springer.com/chapter/10.1007/978-3-540-72079-9_10)]. In summary, a content-based recommender system utilizes the features of product(s) in order to recommend other product(s) similar to what a user has liked, or purchased, or used.\n",
    "\n",
    "### Dataset Gathering and Cleaning\n",
    "Downloading all Wikipedia articles from the webstie as you have suggested comes to 78 GB of data when unzipped. So, as the source data I gather a series of articles from Wikipedia. In order to gather such articles I use __wikipedia API__ for Python that can be accessed through [this link](https://link.springer.com/chapter/10.1007/978-3-540-72079-9_10). This API could be installed using the 'pip', as the standard package-management system for Python, using the command <code>pip install wikipedia</code>. In this work I gather a random set of __ONLY__ <code>num_random_articles=25000</code> wikipedia articles. Alghout, if I had more time and resources, I could have gathered, more and more articles to further saturate the corpus pool with multiple articles.\n",
    "\n",
    "As a very simple cleaning I get read of non-contextual components in the text, e.g. '\\n' and etc. For each article I gather the whole summary of the article provided by the wikipedia API. Such a summary usually includes the top most chunk of text that you observe generally when you open a wikipedia page.\n",
    "\n",
    "### Featurization\n",
    "\n",
    "In order to generate an encoded feature-like representation of each article, I create __vector__ representations using the __TF-IDF approach__. Naming each article as 'document' and the whole set of articles as 'corpus', the TF-IDF approach provides a metric which captures the Term Frequency (TF) in the documents as well as the Inverse Document Frequency (IDF) in the corpus. The Term Frequency (TF) is the frequency of different words that appear in a document and the Inverse Document Frequency (IDF) is the inverse of document frequecy among the whole corpus of all documents. In other words, TF measures how often a word appears within a specific document while IDF measures how rare a word appears within the corpus. TFIDF in particular suppresses the dominance (overinflucence) of high freqeuncy words when it comes to determining their importance. Moreover, in simple words, a word is considered important in a document if, it occurs a lot in that document, but rarely in other documents within the corpus. For further reading about TF-IDF please refer to Refs. [[2](https://dl.acm.org/doi/abs/10.1145/1361684.1361686),[3](https://ieeexplore.ieee.org/abstract/document/7754750/)].\n",
    "\n",
    "Accordingly, I create a set of feature vectors corresponding to each wikipedia article using the TF-IDF approach. TF-IDF vectorization package could be accessed from the scikit-learn Python package through [this link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). I convert all the characters to lowercase before tokenizing in TF-IDF, consider features at the word level not character n-grams, and consider the top <code>num_tfidf_features=100</code> words ordered by term frequency across the corpus as the TF-IDF features.\n",
    "\n",
    "### Measure of Similarity\n",
    "\n",
    "In order to identify the recommended articles within the gathered corpus of wikipedia articles that are similar to an input article I use the 'Cosine Similarity' as the similarity measure between the TF-IDF vectors that characterize the features attributed to each wikipedia article. Comparing the Cosine Similary between the TF-IDF vector of the input article and the TF-IDF vectors all the articles in the corpus, I then recommend the top <code>num_recommended_articles=10</code> articles to user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing/installing the required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the required libraries and packages\n",
    "# use the below command to install any missing packages, e.g. 'wikipedia' API package\n",
    "# _=pip3 install wikipedia\n",
    "\n",
    "# importing the required libraries and packages\n",
    "import os\n",
    "import pickle\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf_vectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warning suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the input parameters provided by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_articles       = 25000      # number of wikipedia articles to be gathered as the recommendation corpus pool\n",
    "load_or_download_articles = 'download' # choices: {download,load}\n",
    "data_folder               = \"./data\"   # directory folder for saving/loading the .pkl file of wikipedia articles\n",
    "num_tfidf_features        = 1000        # number of tfidf features\n",
    "input_article_link        = \"https://en.wikipedia.org/wiki/Donald_Trump\"  # input article for recommendation\n",
    "num_recommended_articles  = 10\n",
    "n_gram_range              = (1,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather random wikipedia articles (titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of random article titles: 25000\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "        \n",
    "if load_or_download_articles == 'download':\n",
    "    random_articles_titles=[s for i in [wikipedia.random(500) for i in range(int(num_random_articles//500))] for s in i]\n",
    "\n",
    "    with open(data_folder+'/random_articles_titles.pkl', 'wb') as file:\n",
    "        pickle.dump(random_articles_titles, file)\n",
    "        \n",
    "elif load_or_download_articles == 'load':\n",
    "    with open(data_folder+'/random_articles_titles.pkl', 'rb') as file:\n",
    "        random_articles_titles = pickle.load(file)\n",
    "\n",
    "print(\"number of random article titles:\", len(set(random_articles_titles)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the summary text for the wikipedia titles randomly gathered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "500 articles have been loaded!\n",
      "most recent sample title is: The Power of Buddhism\n",
      "===============\n",
      "===============\n",
      "1000 articles have been loaded!\n",
      "most recent sample title is: Amorbia effoetana\n",
      "===============\n",
      "===============\n",
      "1500 articles have been loaded!\n",
      "most recent sample title is: Gregory Fernando Pappas\n",
      "===============\n",
      "===============\n",
      "2000 articles have been loaded!\n",
      "most recent sample title is: Valdosta Millionaires\n",
      "===============\n",
      "===============\n",
      "2500 articles have been loaded!\n",
      "most recent sample title is: Marcello Visconti di Modrone\n",
      "===============\n",
      "===============\n",
      "3000 articles have been loaded!\n",
      "most recent sample title is: Édouard Cissé\n",
      "===============\n",
      "===============\n",
      "3500 articles have been loaded!\n",
      "most recent sample title is: Reign of Fire\n",
      "===============\n",
      "===============\n",
      "4000 articles have been loaded!\n",
      "most recent sample title is: Mahonia Hall\n",
      "===============\n",
      "===============\n",
      "4500 articles have been loaded!\n",
      "most recent sample title is: Cleora scriptaria\n",
      "===============\n",
      "===============\n",
      "5000 articles have been loaded!\n",
      "most recent sample title is: Rodney McLeod\n",
      "===============\n",
      "===============\n",
      "5500 articles have been loaded!\n",
      "most recent sample title is: Listed buildings in Jersey\n",
      "===============\n",
      "===============\n",
      "6000 articles have been loaded!\n",
      "most recent sample title is: William Wilson (artist)\n",
      "===============\n",
      "===============\n",
      "6500 articles have been loaded!\n",
      "most recent sample title is: Giulio Tonti\n",
      "===============\n",
      "===============\n",
      "7000 articles have been loaded!\n",
      "most recent sample title is: Opening Doors (Desperate Housewives)\n",
      "===============\n",
      "===============\n",
      "7500 articles have been loaded!\n",
      "most recent sample title is: Zinstall Easy Transfer\n",
      "===============\n",
      "===============\n",
      "8000 articles have been loaded!\n",
      "most recent sample title is: Inger S. Enger\n",
      "===============\n",
      "===============\n",
      "8500 articles have been loaded!\n",
      "most recent sample title is: Ilex amelanchier\n",
      "===============\n",
      "===============\n",
      "9000 articles have been loaded!\n",
      "most recent sample title is: Kawasaki KE100\n",
      "===============\n",
      "===============\n",
      "9500 articles have been loaded!\n",
      "most recent sample title is: Sree Narayana College, Nattika\n",
      "===============\n",
      "===============\n",
      "10000 articles have been loaded!\n",
      "most recent sample title is: Krzysztof Lenk\n",
      "===============\n",
      "===============\n",
      "10500 articles have been loaded!\n",
      "most recent sample title is: Canton of Le Val du Dropt\n",
      "===============\n",
      "===============\n",
      "11000 articles have been loaded!\n",
      "most recent sample title is: GlobalGiving\n",
      "===============\n",
      "===============\n",
      "11500 articles have been loaded!\n",
      "most recent sample title is: 2013 Finnish Figure Skating Championships\n",
      "===============\n",
      "===============\n",
      "12000 articles have been loaded!\n",
      "most recent sample title is: 1013\n",
      "===============\n",
      "===============\n",
      "12500 articles have been loaded!\n",
      "most recent sample title is: Impiety (band)\n",
      "===============\n",
      "===============\n",
      "13000 articles have been loaded!\n",
      "most recent sample title is: Eburodacrys vittata\n",
      "===============\n",
      "===============\n",
      "13500 articles have been loaded!\n",
      "most recent sample title is: Mushtaq Hussain Khan\n",
      "===============\n",
      "===============\n",
      "14000 articles have been loaded!\n",
      "most recent sample title is: Madonna della Ceriola\n",
      "===============\n",
      "===============\n",
      "14500 articles have been loaded!\n",
      "most recent sample title is: American Eagles women's basketball\n",
      "===============\n",
      "===============\n",
      "15000 articles have been loaded!\n",
      "most recent sample title is: Blame It on the Alcohol\n",
      "===============\n",
      "===============\n",
      "15500 articles have been loaded!\n",
      "most recent sample title is: Tomorrow's a Killer\n",
      "===============\n",
      "===============\n",
      "16000 articles have been loaded!\n",
      "most recent sample title is: Dzmitry Lebedzew\n",
      "===============\n",
      "===============\n",
      "16500 articles have been loaded!\n",
      "most recent sample title is: U.S. Route 45 in Alabama\n",
      "===============\n",
      "===============\n",
      "17000 articles have been loaded!\n",
      "most recent sample title is: Alexandre De Saedeleer\n",
      "===============\n",
      "===============\n",
      "17500 articles have been loaded!\n",
      "most recent sample title is: Majed Kilani\n",
      "===============\n",
      "===============\n",
      "18000 articles have been loaded!\n",
      "most recent sample title is: Sitka, Indiana\n",
      "===============\n",
      "===============\n",
      "18500 articles have been loaded!\n",
      "most recent sample title is: Kanch Mandir\n",
      "===============\n",
      "===============\n",
      "19000 articles have been loaded!\n",
      "most recent sample title is: Transcendence (2012 film)\n",
      "===============\n",
      "===============\n",
      "19500 articles have been loaded!\n",
      "most recent sample title is: Rex Wockner\n",
      "===============\n",
      "===============\n",
      "20000 articles have been loaded!\n",
      "most recent sample title is: Viktor Kamarzayev\n",
      "===============\n",
      "===============\n",
      "20500 articles have been loaded!\n",
      "most recent sample title is: Trivas\n",
      "===============\n",
      "===============\n",
      "21000 articles have been loaded!\n",
      "most recent sample title is: Bert Gook\n",
      "===============\n",
      "===============\n",
      "21500 articles have been loaded!\n",
      "most recent sample title is: Chełmno Słowieńskie\n",
      "===============\n",
      "===============\n",
      "22000 articles have been loaded!\n",
      "most recent sample title is: Boston Society of Film Critics Award for Best Supporting Actress\n",
      "===============\n",
      "===============\n",
      "22500 articles have been loaded!\n",
      "most recent sample title is: Orders of magnitude (speed)\n",
      "===============\n",
      "===============\n",
      "23000 articles have been loaded!\n",
      "most recent sample title is: Horna / Behexen\n",
      "===============\n",
      "===============\n",
      "23500 articles have been loaded!\n",
      "most recent sample title is: Moody Barn\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "all_titles = random_articles_titles\n",
    "\n",
    "if load_or_download_articles == 'download':\n",
    "    titles_content_map = dict()\n",
    "    counter = 0\n",
    "    for title_num in range(len(all_titles)):  \n",
    "        title = all_titles[title_num]\n",
    "        try:\n",
    "            summary_text = wikipedia.summary(title)\n",
    "        except:\n",
    "            continue\n",
    "        counter+=1\n",
    "        if counter%500==0: \n",
    "            print(\"=\"*15)\n",
    "            print(\"{} articles have been loaded!\".format(counter))  \n",
    "            print(\"most recent sample title is:\",title)\n",
    "            print(\"=\"*15)\n",
    "        summary_text_processed = summary_text.replace(\"\\n\",\"\").replace(\"=\",\"\").replace(\"/\",\"\").replace(\"  \",\" \")\n",
    "        titles_content_map[title] = summary_text_processed\n",
    "        \n",
    "    with open(data_folder+'/titles_content_map.pkl', 'wb') as file:\n",
    "        pickle.dump(titles_content_map, file)\n",
    "    \n",
    "elif load_or_download_articles == 'load':\n",
    "    with open(data_folder+'/titles_content_map.pkl', 'rb') as file:\n",
    "        titles_content_map = pickle.load(file)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the feature vectors of the corpus articles using TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of random article titles after processing: 23578\n",
      "number of tfidf vectorized elements: 1000\n"
     ]
    }
   ],
   "source": [
    "all_titles_contents = list(titles_content_map.values())\n",
    "all_titles = list(titles_content_map.keys())\n",
    "\n",
    "vectorizer = tfidf_vectorizer(input=all_titles_contents, lowercase=True, \n",
    "                              stop_words=\"english\", ngram_range=n_gram_range ,max_features=num_tfidf_features)\n",
    "\n",
    "all_titles_contents_matrix = vectorizer.fit_transform(all_titles_contents)\n",
    "\n",
    "print(\"number of random article titles after processing:\", len(titles_content_map.keys()))\n",
    "print(\"number of tfidf vectorized elements:\", len(vectorizer.get_feature_names()))\n",
    "\n",
    "sim_unigram=cosine_similarity(all_titles_contents_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Featurizing the input article and recommending num_recommended_articles=10 articles similar to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================== \n",
      "The title of the input article is: Donald_Trump\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "The link for the input article is:https://en.wikipedia.org/wiki/Donald_Trump\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "Downloaded summary of the input article is:\n",
      "\n",
      "Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality.Born and raised in Queens, New York City, Trump attended Fordham University for two years and received a bachelor's degree in economics from the Wharton School of the University of Pennsylvania. He became president of his father's real estate business in 1971, renamed it The Trump Organization, and expanded its operations to building or renovating skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. Trump and his businesses have been involved in more than 4,000 state and federal legal actions, including six bankruptcies. He owned the Miss Universe brand of beauty pageants from 1996 to 2015, and produced and hosted the reality television series The Apprentice from 2004 to 2015.Trump's political positions have been described as populist, protectionist, isolationist, and nationalist. He entered the 2016 presidential race as a Republican and was elected in a surprise electoral college victory over Democratic nominee Hillary Clinton while losing the popular vote. He became the oldest first-term U.S. president and the first without prior military or government service. His election and policies have sparked numerous protests. Trump has made many false or misleading statements during his campaign and presidency. The statements have been documented by fact-checkers, and the media have widely described the phenomenon as unprecedented in American politics. Many of his comments and actions have been characterized as racially charged or racist.During his presidency, Trump ordered a travel ban on citizens from several Muslim-majority countries, citing security concerns; after legal challenges, the Supreme Court upheld the policy's third revision. He enacted a tax-cut package for individuals and businesses, rescinding the individual health insurance mandate penalty of the Affordable Care Act, but has failed to repeal and replace the ACA as a whole. He appointed Neil Gorsuch, Brett Kavanaugh and Amy Coney Barrett to the Supreme Court. In foreign policy, Trump has pursued an America First agenda, withdrawing the U.S. from the Trans-Pacific Partnership trade negotiations, the Paris Agreement on climate change, and the Iran nuclear deal. He imposed import tariffs which triggered a trade war with China, moved the U.S. embassy in Israel to Jerusalem and withdrew U.S. troops from northern Syria. Trump met three times with North Korean leader Kim Jong-un, but talks on denuclearization broke down in 2019.A special counsel investigation led by Robert Mueller found that Trump and his campaign welcomed and encouraged Russian interference in the 2016 presidential election under the belief that it would be politically advantageous, but did not find sufficient evidence to press charges of criminal conspiracy or coordination with Russia. Mueller also investigated Trump for obstruction of justice, and his report neither indicted nor exonerated Trump on that offense. After Trump solicited Ukraine to investigate his political rival Joe Biden, the House of Representatives impeached him in December 2019 for abuse of power and obstruction of Congress. The Senate acquitted him of both charges in February 2020.Trump reacted slowly to the COVID-19 pandemic; he downplayed the threat, ignored or contradicted many recommendations from health officials, and promoted false information about unproven treatments and the availability of testing. Trump ran for re-election in the 2020 U.S. presidential election for the Republican Party.\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "Decoded TF-IDF form of the input article summary is:\n",
      "\n",
      "['000', '14', '19', '1946', '1971', '1996', '2004', '2015', '2016', '2019', '2020', 'act', 'america', 'american', 'appointed', 'born', 'building', 'business', 'campaign', 'china', 'city', 'college', 'congress', 'countries', 'court', 'current', 'december', 'degree', 'democratic', 'described', 'did', 'elected', 'election', 'father', 'february', 'federal', 'foreign', 'government', 'health', 'house', 'including', 'individual', 'information', 'involved', 'iran', 'john', 'june', 'korean', 'later', 'leader', 'led', 'majority', 'media', 'military', 'moved', 'new', 'new york', 'new york city', 'north', 'northern', 'numerous', 'operations', 'organization', 'owned', 'pacific', 'paris', 'party', 'pennsylvania', 'policy', 'political', 'popular', 'power', 'president', 'prior', 'produced', 'race', 'ran', 'real', 'received', 'representatives', 'republican', 'robert', 'russia', 'russian', 'school', 'senate', 'series', 'service', 'special', 'started', 'state', 'states', 'television', 'television series', 'term', 'times', 'trade', 'united', 'united states', 'university', 'various', 'war', 'widely', 'years', 'york', 'york city']\n",
      "====================================================================================================\n",
      "Titles of recommended articles: \n",
      "\n",
      "['Bobby Jindal 2016 presidential campaign', '1996 United States Senate special election in Kansas', 'June 2015 Justice and Development Party election campaign', 'James W. Monroe', '1836 United States presidential election in Missouri', '2002 United States Senate election in Minnesota', '1952 United States presidential election in Louisiana', 'Gary Warren', '1980 presidential election', '1992 United States presidential election in Idaho']\n",
      "====================================================================================================\n",
      "Recommended article no. 1 \n",
      "\n",
      "Title:\n",
      ">> Bobby Jindal 2016 presidential campaign\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/Bobby_Jindal_2016_presidential_campaign\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['17', '2012', '2015', '2016', '24', 'american', 'announced', 'came', 'campaign', 'election', 'following', 'governor', 'indian', 'june', 'november', 'president', 'republican', 'run', 'states', 'united', 'united states', 'years']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 2 \n",
      "\n",
      "Title:\n",
      ">> 1996 United States Senate special election in Kansas\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/1996_United_States_Senate_special_election_in_Kansas\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['11', '1996', 'appointed', 'campaign', 'class', 'defeated', 'election', 'general', 'governor', 'held', 'june', 'leader', 'majority', 'november', 'order', 'president', 'primary', 'republican', 'seat', 'senate', 'special', 'state', 'states', 'united', 'united states', 'went', 'win']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 3 \n",
      "\n",
      "Title:\n",
      ">> June 2015 Justice and Development Party election campaign\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/June_2015_Justice_and_Development_Party_election_campaign\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['10', '1999', '2001', '2002', '2011', '2014', '2015', '40', 'according', 'addition', 'announced', 'august', 'called', 'campaign', 'democratic', 'despite', 'development', 'different', 'eastern', 'elected', 'election', 'eventually', 'executive', 'final', 'founded', 'fourth', 'general', 'government', 'having', 'june', 'leader', 'leading', 'main', 'majority', 'movement', 'new', 'non', 'november', 'official', 'officially', 'opened', 'opening', 'parliament', 'party', 'place', 'placed', 'political', 'power', 'president', 'prior', 'public', 'result', 'september', 'south', 'standard', 'state', 'support', 'time', 'way', 'widely', 'won']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 4 \n",
      "\n",
      "Title:\n",
      ">> James W. Monroe\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/James_W._Monroe\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['28', '30', 'administration', 'africa', 'american', 'announced', 'april', 'army', 'best', 'best known', 'border', 'born', 'britain', 'capital', 'city', 'congress', 'countries', 'county', 'defeated', 'democratic', 'died', 'election', 'empire', 'era', 'established', 'european', 'family', 'father', 'fifth', 'financial', 'florida', 'following', 'force', 'foreign', 'france', 'generally', 'george', 'good', 'governor', 'independent', 'james', 'john', 'joined', 'july', 'king', 'known', 'later', 'law', 'leader', 'left', 'member', 'named', 'national', 'new', 'new york', 'new york city', 'north', 'party', 'policy', 'political', 'president', 'republican', 'secretary', 'senate', 'served', 'signed', 'society', 'spain', 'spanish', 'special', 'state', 'states', 'thomas', 'united', 'united states', 'virginia', 'war', 'washington', 'western', 'won', 'york', 'york city']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 5 \n",
      "\n",
      "Title:\n",
      ">> 1836 United States presidential election in Missouri\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/1836_United_States_presidential_election_in_Missouri\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['1988', 'border', 'came', 'carolina', 'close', 'december', 'defeated', 'democratic', 'despite', 'elected', 'election', 'end', 'event', 'finished', 'generally', 'held', 'henry', 'house', 'independent', 'just', 'led', 'majority', 'massachusetts', 'multiple', 'national', 'north', 'northern', 'november', 'ohio', 'party', 'pennsylvania', 'performance', 'person', 'points', 'popular', 'president', 'primarily', 'ran', 'representatives', 'richard', 'running', 'second', 'senate', 'short', 'single', 'south', 'southern', 'state', 'states', 'success', 'united', 'united states', 'van', 'virginia', 'white', 'william', 'win', 'won']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 6 \n",
      "\n",
      "Title:\n",
      ">> 2002 United States Senate election in Minnesota\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/2002_United_States_Senate_election_in_Minnesota\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['1964', '1976', '1984', '1996', '2002', '2020', 'appointed', 'day', 'days', 'democratic', 'died', 'election', 'governor', 'held', 'lost', 'mayor', 'november', 'office', 'party', 'paul', 'place', 'president', 'previously', 'republican', 'running', 'saint', 'seat', 'senate', 'states', 'term', 'took', 'took place', 'united', 'united states', 'won']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 7 \n",
      "\n",
      "Title:\n",
      ">> 1952 United States presidential election in Louisiana\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/1952_United_States_presidential_election_in_Louisiana\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['10', '1952', '1968', '2016', 'college', 'columbia', 'democratic', 'election', 'illinois', 'john', 'new', 'new york', 'november', 'official', 'parish', 'place', 'popular', 'president', 'representatives', 'republican', 'richard', 'running', 'state', 'states', 'took', 'took place', 'union', 'united', 'united states', 'university', 'win', 'winning', 'won', 'york']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 8 \n",
      "\n",
      "Title:\n",
      ">> Gary Warren\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/Gary_Warren\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['1936', '1958', '1972', '1974', '1975', '1980', '1984', '1986', '1987', '1988', '28', 'act', 'american', 'books', 'born', 'campaign', 'career', 'children', 'church', 'committee', 'council', 'december', 'defeated', 'democratic', 'early', 'election', 'force', 'general', 'george', 'including', 'ireland', 'island', 'james', 'known', 'law', 'led', 'married', 'northern', 'november', 'peter', 'politician', 'post', 'practice', 'president', 'private', 'public', 'race', 'represented', 'republican', 'returned', 'richard', 'roles', 'school', 'senate', 'served', 'special', 'states', 'successful', 'united', 'united states', 'university', 'variety', 'widely', 'winning', 'written']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 9 \n",
      "\n",
      "Title:\n",
      ">> 1980 presidential election\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/1980_presidential_election\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['100', '1955', '1976', '1980', '2016', '50', 'age', 'best', 'brother', 'california', 'campaign', 'control', 'defeated', 'democratic', 'economic', 'elected', 'election', 'elections', 'end', 'era', 'following', 'george', 'governor', 'held', 'high', 'highest', 'home', 'illinois', 'independent', 'iran', 'john', 'just', 'large', 'majority', 'national', 'nominated', 'non', 'november', 'number', 'officially', 'party', 'performed', 'person', 'political', 'popular', 'president', 'previously', 'race', 'received', 'remained', 'republican', 'right', 'running', 'second', 'senate', 'served', 'social', 'states', 'term', 'texas', 'time', 'united', 'united states', 'washington', 'wing', 'won', 'years']\n",
      "\n",
      "====================================================================================================\n",
      "Recommended article no. 10 \n",
      "\n",
      "Title:\n",
      ">> 1992 United States presidential election in Idaho\n",
      "\n",
      "Link:\n",
      ">> https://en.wikipedia.org/wiki/1992_United_States_presidential_election_in_Idaho\n",
      "\n",
      "Decoded TF-IDF form of the recommended article: \n",
      ">> ['100', '1992', '2016', '27', '28', 'close', 'college', 'county', 'democratic', 'did', 'election', 'elections', 'finished', 'fourth', 'george', 'governor', 'national', 'november', 'place', 'popular', 'president', 'prior', 'representatives', 'second', 'state', 'states', 'texas', 'took', 'took place', 'united', 'united states', 'won', 'years']\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "wiki_article_link_format = input_article_link.split(\"/\").copy()\n",
    "input_article_title = input_article_link.split('/')[-1]\n",
    "print(\"=\"*100, \"\\nThe title of the input article is: {}\".format(input_article_title))\n",
    "print(\"-\"*100, \"\\nThe link for the input article is:{}\".format(input_article_link))\n",
    "summary_text = wikipedia.summary(input_article_title)\n",
    "summary_text_processed = summary_text.replace(\"\\n\",\"\").replace(\"=\",\"\").replace(\"  \",\" \")\n",
    "print(\"-\"*100,\"\\nDownloaded summary of the input article is:\\n\\n{}\".format(summary_text_processed))\n",
    "\n",
    "summary_text_processed_vec = vectorizer.transform([summary_text_processed])\n",
    "summary_text_processed_decoded = [vectorizer.get_feature_names()[i] \n",
    "                                  for i in range(len(vectorizer.get_feature_names())) \n",
    "                                  if summary_text_processed_vec.todense()[0,i]!=0.0]\n",
    "\n",
    "print(\"-\"*100,\"\\nDecoded TF-IDF form of the input article summary is:\\n\\n{}\".format(summary_text_processed_decoded))\n",
    "\n",
    "cos_sim = cosine_similarity(summary_text_processed_vec,all_titles_contents_matrix)\n",
    "rec_titles_elems = cos_sim.argsort()[0][-num_recommended_articles:][::-1]\n",
    "cos_sim.sort()\n",
    "rec_titles_cos_sim = cos_sim[0][-num_recommended_articles:][::-1]\n",
    "rec_titles = [all_titles[elem] for elem in rec_titles_elems]\n",
    "print(\"=\"*100)\n",
    "print(\"Titles of recommended articles: \\n\\n{}\".format(rec_titles))\n",
    "\n",
    "print(\"=\"*100)\n",
    "for i in range(len(rec_titles_elems)):\n",
    "    rec_title_elem = rec_titles_elems[i]\n",
    "    print(\"Recommended article no. {} \\n\".format(i+1))\n",
    "    print(\"Title:\\n>> {}\\n\".format(rec_titles[i]))\n",
    "    link = \"/\".join(wiki_article_link_format[:-1]+[\"_\".join(rec_titles[i].split(\" \"))])\n",
    "    print(\"Link:\\n>> {}\\n\".format(link))\n",
    "    rec_title_vec = all_titles_contents_matrix[rec_title_elem][:]\n",
    "    rec_title_decoded = [vectorizer.get_feature_names()[i] \n",
    "                         for i in range(len(vectorizer.get_feature_names())) \n",
    "                         if rec_title_vec.todense()[0,i]!=0.0]\n",
    "    print(\"Decoded TF-IDF form of the recommended article: \\n>> {}\\n\".format(rec_title_decoded))\n",
    "    # print(\"Cosine Similarity:\\n>> {}\\n\".format(rec_titles_cos_sim[i]))\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Above as an example I have considered the wikipedia page of [Donald Trump](https://en.wikipedia.org/wiki/Donald_Trump) as the input. As I have outputed above, words such as \"president, election, united states, america, new york city, democratic, political, government, senate, congress, elected\" happen to characterize the feature vector of the input article of 'Donald_Trump'.\n",
    "\n",
    "Among the top 10 recommended articles, we see wikipages of for instance *'Bobby Jindal 2016 presidential campaign', '1996 United States Senate special election in Kansas', 'June 2015 Justice and Development Party election campaign', 'James W. Monroe', '1836 United States presidential election in Missouri', '2002 United States Senate election in Minnesota', '1952 United States presidential election in Louisiana', 'Gary Warren', '1980 presidential election', '1992 United States presidential election in Idaho'* which all belong to the topics about the United States, election, US politics, presidential related matters, and etc. Moreover, I have also outputted for each recommended article the corresponding (dense and decoded) feature vectors\n",
    "\n",
    "Accordingly, the proposed recommendation system performs reasonably well, however, in order to further improve it, and make it more robust we can consider some options as stated in the below section about __Future Work__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "#### There are ways to further imporve the presented recommendation platform. Below I provide a few insights about such potential directions:\n",
    "\n",
    "1. Increase the number of collected articles for the recommendation pool\n",
    " * Above I have used <code>num_random_articles=25000</code> wikipedia articles as the corpus pool for my recommendation system. Certainly, adding more and more articles would lead to finding better matches for the articles to be recommended to the user.\n",
    " \n",
    " \n",
    "2. Gather articles (topics) relavant to the recommendation context, e.g. is the recommendation context more around politics, sports, foods, or etc.\n",
    " * This could specially be helpful if we are building the recommendation system for a specific application or an intended audience - For instance, if we intend to build recommendation system for chemists we better have as many chemical engineering related articles as possible in our corpus pool.\n",
    " \n",
    " \n",
    "3. Try alternative featurization techniques/variations. A couple of ideas could be:\n",
    "   * Tuning the length of n-grams that we use for featurizing the articles - above I have picked n-grams of n=1 to n=5 as <code>n_gram_range=(1,5)</code>.\n",
    "   * Above I have featurized each wikipedia article using its summary text, an alternative would be using some pre-trained word-vectors such as 'word2vec' in order to create vectorized features of only the __Titles__ instead of the body text, and experiment to see if any improvement could be achieved. \n",
    "   * Using abstractive text summarization models to summarize the wikipedia articles first, then use those summarized texts for creating feature vectors - See for instance my work on how text summarization could help with text understanding and classification - [[4.] Soheil Esmaeilzadeh et al. - Neural Abstractive Text Summarization and Fake News Detection](https://arxiv.org/abs/1904.00788).\n",
    "   * Above I have used <code>num_tfidf_features=1000</code> number of features - Tunning the number of features impacts the absolute value of cosine similarities as well as the level of information retrieval/extraction from the articles. An application specific tuning of this parameter could further improve the recommendation framework.\n",
    "   \n",
    "   \n",
    "* In addition to looking at the contents of wikipages, we can include some metadata associated to each article, e.g. topic category, year of publish, length, language, etc.\n",
    "\n",
    "\n",
    "* We can tune (limit) the number of sentences that are extracted from each wiki page from its summary and avoid the potential bias that might be caused if summary of some articles end up being longer (shorter) than the others.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
